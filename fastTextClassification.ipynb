{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Generating Tags for Content using Amazon SageMaker BlazingText with fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label Text classification is one of the fundamental tasks in Natural Language Processing (NLP) applications. In this demo, We will build a fastText model to predict the tag of question about programming and then deploy the model on SageMaker Hosting Services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup kaggle cli and download dataset\n",
    "In this demo, We will use the [10% of Stack Overflow Q&A dataset](https://medium.com/r/?url=https%3A%2F%2Fwww.kaggle.com%2Fstackoverflow%2Fstacksample). The dataset include:\n",
    "- Questions contains the title, body, creation date, closed date (if applicable), score, and owner ID for all non-deleted Stack Overflow questions whose Id is a multiple of 10.\n",
    "- Answers contains the body, creation date, score, and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table.\n",
    "- Tags contains the tags on each of these questions\n",
    "\n",
    "We only need Questions and Tags to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /home/ec2-user/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv .kaggle/kaggle.json /home/ec2-user/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 /home/ec2-user/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download stackoverflow/stacksample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -l stacksample.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -j stacksample.zip Questions.csv -d stacksample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -j stacksample.zip Tags.csv -d stacksample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,OwnerUserId,CreationDate,ClosedDate,Score,Title,Body\n",
      "80,26,2008-08-01T13:57:07Z,NA,26,SQLStatement.execute() - multiple queries in one statement,\"<p>I've written a database generation script in <a href=\"\"http://en.wikipedia.org/wiki/SQL\"\">SQL</a> and want to execute it in my <a href=\"\"http://en.wikipedia.org/wiki/Adobe_Integrated_Runtime\"\">Adobe AIR</a> application:</p>\n",
      "Id,Tag\n",
      "80,flex\n",
      "80,actionscript-3\n",
      "80,air\n",
      "90,svn\n",
      "90,tortoisesvn\n",
      "90,branch\n",
      "90,branching-and-merging\n",
      "120,sql\n",
      "120,asp.net\n"
     ]
    }
   ],
   "source": [
    "!head stacksample/Questions.csv -n 2\n",
    "!head stacksample/Tags.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): \n",
    "        return text\n",
    "    def cleanhtml(raw_html):\n",
    "        cleanr = re.compile('<[^>]+>')\n",
    "        cleantext = re.sub(cleanr, '', raw_html)\n",
    "        return cleantext\n",
    "    def replace_link(match):\n",
    "        return '' if re.match('[a-z]+://', match.group(1)) else match.group(1)\n",
    "    def removeContractions(raw_text):\n",
    "        CONTRACTIONS = {\"mayn't\":\"may not\", \"may've\":\"may have\",\"isn't\":\"is not\",\"wasn't\":\"was not\",\"'ll\":\" will\",\"'have\": \"have\"}\n",
    "        raw_text = raw_text.replace(\"’\",\"'\")\n",
    "        words = raw_text.split()\n",
    "        reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "        raw_text = \" \".join(reformed)\n",
    "        return raw_text\n",
    "    text = cleanhtml(text)\n",
    "    #text = removeContractions(text)\n",
    "    text = re.sub('<pre><code>.*?</code></pre>', '', text)\n",
    "    text = re.sub('<a[^>]+>(.*)</a>', replace_link, text)\n",
    "    #Remove hashtags\n",
    "    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", text).split())\n",
    "    #Remove punctuations\n",
    "    text = ' '.join(re.sub(\"[\\.\\,\\(\\)\\{\\}\\[\\]\\`\\'\\!\\?\\:\\;\\-\\=]\", \" \", text).split())\n",
    "    #text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    #text = text.lower()\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "QuestionsFile = \"stacksample/Questions.csv\"\n",
    "chunksize = 20000\n",
    "\n",
    "df = None\n",
    "for ratings in pd.read_csv(QuestionsFile, names=['id', 'title', 'body'], encoding = 'ISO-8859-1',  header=None , usecols=[0,5,6],error_bad_lines = False, chunksize=chunksize):\n",
    "    if df is None:\n",
    "        df = ratings.copy()\n",
    "    else:\n",
    "        df.append(ratings)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[1:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=.5, replace=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "TagsFile = \"stacksample/Tags.csv\"\n",
    "chunksize = 20000\n",
    "df_tags = None\n",
    "for ratings in pd.read_csv(TagsFile, names=['id', 'tag'], header=None , chunksize=chunksize):\n",
    "    if df_tags is None:\n",
    "        df_tags = ratings.copy()\n",
    "    else:\n",
    "        df_tags.append(ratings)\n",
    "\n",
    "df_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = df_tags[1:]\n",
    "df_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df.values\n",
    "all_rows=[]\n",
    "\n",
    "for index, row in enumerate(questions):\n",
    "    title = clean_text(row[1])\n",
    "    tag_ids = [ tag[1] for tag_idx, tag in enumerate(tags) if tag[0]  == row[0] ]\n",
    "    if(len(tag_ids)>0): \n",
    "        all_rows.append({\"title\":title, \"tags\":tag_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def preprocess(rows,output_file):\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    with open(output_file, \"w\") as txt_file:\n",
    "        for line in transformed_rows:\n",
    "            txt_file.write(\" \".join(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input file is formatted in a way that each line contain a single sentence and the corresponding label(s) prefixed by \\_\\_label__,  i.e. \\_\\_label__database \\_\\_label__oracle How to edit sessions parameters on Oracle 10g XE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = [\"__label__\" + str(tag) for tag in row[\"tags\"] if tag]\n",
    "    label = \" \".join(map(str, label))\n",
    "    cur_row.append(str(label))\n",
    "    cur_row.extend(nltk.word_tokenize(row[\"title\"]))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(all_rows[:1200], 'stackoverflow.train')    \n",
    "preprocess(all_rows[1200:], 'stackoverflow.validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/facebookresearch/fastText/archive/v0.2.0.zip\n",
    "!unzip v0.2.0.zip\n",
    "!cd fastText-0.2.0 && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd fastText-0.2.0 && ./fasttext supervised -input \"../stackoverflow.train\" -output stack_model -lr 0.5 -epoch 25 -minCount 5 -wordNgrams 2 -loss ova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to directly test classifier interactively, by running the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd fastText-0.2.0 && ./fasttext test stack_model.bin \"../stackoverflow.validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket = sess.default_bucket() \n",
    "prefix = 'blazingtext/stackoverflow' \n",
    "\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf model.tar.gz fastText-0.2.0/stack_model.bin\n",
    "model_location = sess.upload_data(\"model.tar.gz\", bucket=bucket, key_prefix=prefix)\n",
    "print(model_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting\n",
    "\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stackoverflow = sagemaker.Model(model_data=model_location, image=container, role=role, sagemaker_session=sess)\n",
    "stackoverflow.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')\n",
    "predictor = sagemaker.RealTimePredictor(endpoint=stackoverflow.endpoint_name, \n",
    "                                   sagemaker_session=sess,\n",
    "                                   serializer=json.dumps,\n",
    "                                   deserializer=sagemaker.predictor.json_deserializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now that the trained model is deployed at an endpoint that is up-and-running, we can use this endpoint for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"How can I refresh a page with jQuery\"\n",
    "\n",
    "payload = {\"instances\" : [sentence],\"configuration\": {\"k\":3}}\n",
    "predictions = predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "predictions_copy = copy.deepcopy(predictions) \n",
    "for output in predictions_copy:\n",
    "    for index,label in enumerate(output['label']):\n",
    "        label_title = label[9:]\n",
    "        prob = float(output[\"prob\"][index])\n",
    "        print(f\"{label_title}, {prob} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
